{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from crewai import Agent, Task, Crew, Process\n",
    "from langchain_openai import ChatOpenAI\n",
    "import json\n",
    "\n",
    "# Load configuration\n",
    "with open('config.json', 'r') as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "# Initialize OpenAI client\n",
    "llm = ChatOpenAI(\n",
    "    api_key=config['apiKey'],\n",
    "    model=config['model'],\n",
    "    base_url=config['baseURL']\n",
    ")\n",
    "\n",
    "# Define the agents\n",
    "orchestrator = Agent(\n",
    "    role='Project Manager and Central Planner',\n",
    "    goal='Break down objectives into executable steps and manage the overall workflow',\n",
    "    backstory=\"\"\"You are an experienced project manager specializing in data science projects.\n",
    "    Your expertise lies in breaking down complex objectives into manageable steps and ensuring\n",
    "    the team stays on track to achieve the goals.\"\"\",\n",
    "    verbose=True,\n",
    "    llm=llm,\n",
    "    allow_delegation=True\n",
    ")\n",
    "\n",
    "coder = Agent(\n",
    "    role='Python Code Generator and Executor',\n",
    "    goal='Generate and execute Python code for specific tasks',\n",
    "    backstory=\"\"\"You are an expert Python programmer with deep knowledge in data science,\n",
    "    machine learning, and data visualization. You excel at writing clean, efficient code\n",
    "    that accomplishes specific tasks.\"\"\",\n",
    "    verbose=True,\n",
    "    llm=llm\n",
    ")\n",
    "\n",
    "reflector = Agent(\n",
    "    role='Code Analysis and Decision Maker',\n",
    "    goal='Analyze code execution results and determine next steps',\n",
    "    backstory=\"\"\"You are a meticulous code reviewer and analyst. Your expertise lies in\n",
    "    evaluating code execution results, identifying issues, and making informed decisions\n",
    "    about the next steps in the workflow.\"\"\",\n",
    "    verbose=True,\n",
    "    llm=llm\n",
    ")\n",
    "\n",
    "reviewer = Agent(\n",
    "    role='Quality Assurance and Documentation Specialist',\n",
    "    goal='Ensure code quality and document results',\n",
    "    backstory=\"\"\"You are a quality assurance expert with a strong background in data science\n",
    "    best practices. You excel at evaluating code quality and creating clear documentation\n",
    "    of results and findings.\"\"\",\n",
    "    verbose=True,\n",
    "    llm=llm\n",
    ")\n",
    "\n",
    "code_writer = Agent(\n",
    "    role='Notebook Manager',\n",
    "    goal='Manage and update Jupyter notebook content',\n",
    "    backstory=\"\"\"You are a notebook management specialist. Your expertise lies in\n",
    "    organizing and maintaining Jupyter notebooks, ensuring proper documentation\n",
    "    and code organization.\"\"\",\n",
    "    verbose=True,\n",
    "    llm=llm\n",
    ")\n",
    "\n",
    "# Define the tasks\n",
    "def create_tasks(objective: str):\n",
    "    return [\n",
    "        Task(\n",
    "            description=f\"\"\"Analyze the following objective and create a detailed plan:\n",
    "            {objective}\n",
    "            \n",
    "            Break it down into specific, actionable steps that can be executed in a Jupyter notebook.\n",
    "            Consider data loading, processing, analysis, and visualization needs.\"\"\",\n",
    "            agent=orchestrator\n",
    "        ),\n",
    "        Task(\n",
    "            description=\"\"\"Generate and execute Python code for the current step in the plan.\n",
    "            Ensure the code is clean, efficient, and follows best practices.\n",
    "            Include necessary imports and handle potential errors.\"\"\",\n",
    "            agent=coder\n",
    "        ),\n",
    "        Task(\n",
    "            description=\"\"\"Analyze the code execution results:\n",
    "            1. Check if the code ran successfully\n",
    "            2. Verify if the output makes sense\n",
    "            3. Identify any issues or improvements needed\n",
    "            4. Decide whether to:\n",
    "               - Send back to coder for fixes\n",
    "               - Send to reviewer for quality check\n",
    "               - Send to code writer for notebook update\"\"\",\n",
    "            agent=reflector\n",
    "        ),\n",
    "        Task(\n",
    "            description=\"\"\"Review the code and results:\n",
    "            1. Evaluate code quality and best practices\n",
    "            2. Verify if results achieve the intended goal\n",
    "            3. Create a concise summary of what the code does\n",
    "            4. Document key findings and results\n",
    "            5. Provide approval or feedback\"\"\",\n",
    "            agent=reviewer\n",
    "        ),\n",
    "        Task(\n",
    "            description=\"\"\"Update the Jupyter notebook:\n",
    "            1. Add or update cells with the approved code\n",
    "            2. Include markdown cells for documentation\n",
    "            3. Ensure proper organization and flow\n",
    "            4. Save the notebook with all changes\"\"\",\n",
    "            agent=code_writer\n",
    "        )\n",
    "    ]\n",
    "\n",
    "# Create the crew\n",
    "def create_crew(objective: str):\n",
    "    crew = Crew(\n",
    "        agents=[orchestrator, coder, reflector, reviewer, code_writer],\n",
    "        tasks=create_tasks(objective),\n",
    "        verbose=2,\n",
    "        process=Process.sequential  # Tasks will be executed in sequence\n",
    "    )\n",
    "    return crew\n",
    "\n",
    "# # Example usage\n",
    "# if __name__ == \"__main__\":\n",
    "#     objective = \"Create a data analysis notebook that loads a CSV file, performs basic statistical analysis, and creates visualizations\"\n",
    "#     crew = create_crew(objective)\n",
    "#     result = crew.kickoff()\n",
    "#     print(\"\\nCrew Execution Result:\", result) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Create a crew with just the orchestrator\n",
    "\n",
    "from crewai import Agent, Task, Crew, Process, LLM\n",
    "import json\n",
    "import os\n",
    "\n",
    "with open('config.json', 'r') as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "# Set Gemini API key\n",
    "os.environ[\"GEMINI_API_KEY\"] = \"AIzaSyA5M7V30HNVfYOGMtzuLe4Wr9-J_OcroTk\"\n",
    "\n",
    "# Initialize LLM\n",
    "llm = LLM(\n",
    "    model=\"gemini/gemini-2.5-flash-preview-05-20\",\n",
    "    temperature=0.7,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overriding of current TracerProvider is not allowed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mProject Manager and Central Planner\u001b[00m\n",
      "\u001b[95m## Task:\u001b[00m \u001b[92mAnalyze the following objective and create a detailed, step-by-step plan:\n",
      "\n",
      "        OBJECTIVE: Create a data analysis notebook that:\n",
      "    1. Loads a CSV file\n",
      "    2. Performs basic statistical analysis\n",
      "    3. Creates visualizations\n",
      "\n",
      "        Your task is to:\n",
      "        1. Understand the objective thoroughly\n",
      "        2. Break it down into clear, actionable steps\n",
      "        3. For each step, specify:\n",
      "           - What needs to be done\n",
      "           - Required inputs\n",
      "           - Expected outputs\n",
      "           - Success criteria\n",
      "           - Potential challenges\n",
      "\n",
      "        Your plan should be:\n",
      "        - Specific enough for the Coder to implement\n",
      "        - Flexible enough to handle variations\n",
      "        - Clear about dependencies between steps\n",
      "        - Include error handling considerations\n",
      "\n",
      "        Format your response as a clear, numbered list of steps that can be executed sequentially.\n",
      "        Each step should be self-contained but build upon previous steps.\u001b[00m\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mProject Manager and Central Planner\u001b[00m\n",
      "\u001b[95m## Final Answer:\u001b[00m \u001b[92m\n",
      "As Project Manager and Central Planner, I have analyzed the objective to create a data analysis notebook. Below is a detailed, step-by-step plan designed for clear implementation by the Coder, while allowing for necessary flexibility and addressing potential challenges. Each step includes its purpose, inputs, outputs, success criteria, and error handling considerations.\n",
      "\n",
      "---\n",
      "\n",
      "### Project Plan: Data Analysis Notebook Creation\n",
      "\n",
      "**Overall Objective:** Create a data analysis notebook that loads a CSV file, performs basic statistical analysis, and creates visualizations.\n",
      "\n",
      "---\n",
      "\n",
      "**Step 1: Project Setup and Environment Configuration**\n",
      "\n",
      "*   **What needs to be done:**\n",
      "    *   Create a dedicated project directory.\n",
      "    *   Initialize a virtual environment (e.g., `venv` or `conda env`).\n",
      "    *   Install necessary Python libraries: `pandas` for data manipulation, `numpy` for numerical operations, `matplotlib` and `seaborn` for visualization.\n",
      "    *   Create an empty Jupyter Notebook file (e.g., `data_analysis_notebook.ipynb`).\n",
      "*   **Required Inputs:**\n",
      "    *   Python interpreter (pre-installed).\n",
      "    *   Access to a terminal/command prompt.\n",
      "*   **Expected Outputs:**\n",
      "    *   A project directory containing:\n",
      "        *   A configured virtual environment.\n",
      "        *   A `requirements.txt` file listing installed libraries and their versions.\n",
      "        *   An empty Jupyter Notebook file (`data_analysis_notebook.ipynb`).\n",
      "*   **Success Criteria:**\n",
      "    *   The virtual environment is activated successfully.\n",
      "    *   All specified libraries (pandas, numpy, matplotlib, seaborn) can be imported within a new cell in the Jupyter Notebook without errors.\n",
      "    *   The notebook file is accessible and runnable.\n",
      "*   **Potential Challenges:**\n",
      "    *   Conflicting library versions if not using a virtual environment.\n",
      "    *   Internet connectivity issues during package installation.\n",
      "    *   Incorrect path to Python executable or virtual environment activation script.\n",
      "*   **Error Handling Consideration:**\n",
      "    *   Provide clear instructions for virtual environment activation and package installation commands.\n",
      "    *   Suggest `pip install -r requirements.txt` for reproducibility.\n",
      "\n",
      "---\n",
      "\n",
      "**Step 2: Data Loading**\n",
      "\n",
      "*   **What needs to be done:**\n",
      "    *   Write code to load the specified CSV file into a pandas DataFrame.\n",
      "    *   Assume the CSV file is located in a known path (e.g., `data/input.csv`).\n",
      "*   **Required Inputs:**\n",
      "    *   Path to the CSV file (e.g., `./data/sales_data.csv`).\n",
      "    *   Knowledge of any specific CSV parameters (e.g., delimiter, header row presence, encoding). If not specified, assume default `pd.read_csv` behavior.\n",
      "*   **Expected Outputs:**\n",
      "    *   A pandas DataFrame object containing the loaded data.\n",
      "    *   Initial display of DataFrame head (`df.head()`) and information (`df.info()`) within the notebook.\n",
      "*   **Success Criteria:**\n",
      "    *   The `pd.read_csv()` function executes without raising an error.\n",
      "    *   `df.head()` displays the first few rows of data, confirming correct loading and column recognition.\n",
      "    *   `df.info()` shows the expected number of rows and columns, and initial data types.\n",
      "*   **Potential Challenges:**\n",
      "    *   `FileNotFoundError`: Incorrect file path or file not present.\n",
      "    *   `ParserError`: Malformed CSV (e.g., inconsistent delimiters, extra/missing columns in rows).\n",
      "    *   `UnicodeDecodeError`: Incorrect file encoding.\n",
      "    *   Large file size: Memory issues; may require `chunksize` or using Dask.\n",
      "*   **Error Handling Consideration:**\n",
      "    *   Implement a `try-except FileNotFoundError` block to gracefully handle missing files, providing a clear message to check the file path.\n",
      "    *   Suggest specifying `encoding` (e.g., `'utf-8'`, `'latin1'`) if `UnicodeDecodeError` occurs.\n",
      "    *   Advise checking the CSV delimiter (`sep` parameter) if parsing issues arise.\n",
      "\n",
      "---\n",
      "\n",
      "**Step 3: Initial Data Inspection and Cleaning**\n",
      "\n",
      "*   **What needs to be done:**\n",
      "    *   Inspect the DataFrame for missing values (`df.isnull().sum()`).\n",
      "    *   Check for duplicate rows (`df.duplicated().sum()`).\n",
      "    *   Review data types (`df.dtypes` or `df.info()`) and convert if necessary (e.g., object to numeric, object to datetime).\n",
      "    *   Handle missing values (e.g., drop rows/columns, impute with mean/median/mode, or flag). The specific strategy depends on data context, but a default approach (e.g., dropping rows with many missing values, or simple imputation for small counts) should be chosen.\n",
      "    *   Remove duplicate rows if identified.\n",
      "*   **Required Inputs:**\n",
      "    *   The pandas DataFrame loaded in Step 2.\n",
      "*   **Expected Outputs:**\n",
      "    *   A cleaned pandas DataFrame, `df_cleaned`, with appropriate data types and handled missing/duplicate values.\n",
      "    *   Summary outputs showing the state before and after cleaning (e.g., counts of nulls/duplicates).\n",
      "*   **Success Criteria:**\n",
      "    *   `df_cleaned.isnull().sum()` shows zeros (or acceptable low counts) for critical columns.\n",
      "    *   `df_cleaned.duplicated().sum()` shows zero.\n",
      "    *   Data types are consistent with the nature of the data (e.g., numerical columns are `int` or `float`, date columns are `datetime`).\n",
      "    *   The cleaning process is documented via comments or markdown cells.\n",
      "*   **Potential Challenges:**\n",
      "    *   Ambiguous missing value representations (e.g., \"N/A\", \"-\", \"Unknown\" instead of `NaN`).\n",
      "    *   Incorrect data type conversions leading to errors or data loss (e.g., `pd.to_numeric` on non-numeric strings).\n",
      "    *   Deciding on the best strategy for missing data imputation without domain knowledge.\n",
      "    *   Outliers that might require specific handling.\n",
      "*   **Error Handling Consideration:**\n",
      "    *   Use `errors='coerce'` in `pd.to_numeric` to turn non-convertible values into `NaN`, allowing for subsequent missing value handling.\n",
      "    *   Document all cleaning decisions and assumptions made in markdown cells.\n",
      "\n",
      "---\n",
      "\n",
      "**Step 4: Basic Statistical Analysis**\n",
      "\n",
      "*   **What needs to be done:**\n",
      "    *   Calculate descriptive statistics for numerical columns (e.g., mean, median, standard deviation, min, max, quartiles using `df.describe()`).\n",
      "    *   Perform value counts for key categorical columns (`df['column'].value_counts()`).\n",
      "    *   (Optional but recommended) Calculate correlations between numerical columns (`df.corr()`).\n",
      "*   **Required Inputs:**\n",
      "    *   The cleaned pandas DataFrame (`df_cleaned`) from Step 3.\n",
      "*   **Expected Outputs:**\n",
      "    *   Clearly displayed statistical summaries for numerical columns.\n",
      "    *   Frequency tables for categorical columns.\n",
      "    *   (Optional) A correlation matrix or specific correlation values.\n",
      "    *   Markdown cells explaining the insights derived from these statistics.\n",
      "*   **Success Criteria:**\n",
      "    *   `df_cleaned.describe()` output is displayed and shows sensible values for numerical columns.\n",
      "    *   `value_counts()` outputs are displayed for at least 2-3 relevant categorical columns.\n",
      "    *   No errors occur during statistical computations.\n",
      "    *   The output provides a basic understanding of the data's distribution and central tendencies.\n",
      "*   **Potential Challenges:**\n",
      "    *   Columns mistakenly identified as numerical or categorical, leading to incorrect analysis.\n",
      "    *   Extreme outliers skewing mean/standard deviation.\n",
      "    *   Interpreting statistics without domain context.\n",
      "*   **Error Handling Consideration:**\n",
      "    *   Ensure numerical operations are only applied to actual numerical columns. Use `df.select_dtypes(include=np.number)` for robustness.\n",
      "    *   Add comments or markdown cells to highlight any unexpected statistical results (e.g., very high standard deviation, unexpected min/max values).\n",
      "\n",
      "---\n",
      "\n",
      "**Step 5: Data Visualization**\n",
      "\n",
      "*   **What needs to be done:**\n",
      "    *   Create at least three different types of visualizations using `matplotlib` and/or `seaborn`.\n",
      "    *   Suggestions:\n",
      "        *   **Distribution Plot:** Histogram or Box plot for a key numerical variable.\n",
      "        *   **Categorical Distribution:** Bar plot for a key categorical variable's counts or its relation to a numerical variable (e.g., average sales per category).\n",
      "        *   **Relationship Plot:** Scatter plot between two numerical variables, or a heatmap for correlations.\n",
      "    *   Ensure all plots have clear titles, axis labels, and legends where appropriate.\n",
      "    *   Customize plot aesthetics for readability (e.g., figure size).\n",
      "*   **Required Inputs:**\n",
      "    *   The cleaned pandas DataFrame (`df_cleaned`) from Step 3.\n",
      "    *   Selected columns for visualization.\n",
      "*   **Expected Outputs:**\n",
      "    *   Multiple, well-rendered plots displayed inline within the Jupyter Notebook.\n",
      "    *   Plots are visually informative and convey insights.\n",
      "    *   (Optional) Plots saved as image files (e.g., PNG) to a `visualizations/` directory.\n",
      "*   **Success Criteria:**\n",
      "    *   All specified plot types are generated without errors.\n",
      "    *   Plots effectively illustrate distributions, relationships, or trends in the data.\n",
      "    *   Titles and labels are descriptive and easy to understand.\n",
      "    *   Plots are legible and not overly cluttered.\n",
      "*   **Potential Challenges:**\n",
      "    *   Choosing the most appropriate plot type for the data and insights.\n",
      "    *   Overplotting or unreadable plots with large datasets or many categories.\n",
      "    *   Misleading visualizations due to incorrect scaling or data aggregation.\n",
      "    *   Customization difficulties (e.g., rotating labels, adjusting figure size).\n",
      "*   **Error Handling Consideration:**\n",
      "    *   Check for empty or all-`NaN` columns before attempting to plot them.\n",
      "    *   Provide examples of common plot types and let the coder choose based on data characteristics.\n",
      "    *   Suggest using `plt.tight_layout()` to prevent labels from overlapping.\n",
      "\n",
      "---\n",
      "\n",
      "**Step 6: Notebook Documentation and Finalization**\n",
      "\n",
      "*   **What needs to be done:**\n",
      "    *   Add comprehensive markdown cells throughout the notebook to explain each step, the rationale behind decisions (e.g., cleaning choices), and interpretations of findings.\n",
      "    *   Add comments to complex or non-obvious code sections.\n",
      "    *   Ensure a logical flow from top to bottom.\n",
      "    *   Summarize key findings and insights at the end of the notebook.\n",
      "    *   Review the entire notebook for clarity, correctness, and reproducibility.\n",
      "*   **Required Inputs:**\n",
      "    *   The completed data analysis notebook (`data_analysis_notebook.ipynb`).\n",
      "*   **Expected Outputs:**\n",
      "    *   A self-contained, well-documented, and executable Jupyter Notebook.\n",
      "    *   A clear narrative of the data analysis process and its outcomes.\n",
      "*   **Success Criteria:**\n",
      "    *   The notebook can be executed from start to finish without errors.\n",
      "    *   Anyone unfamiliar with the project can understand the data, the analysis steps, and the conclusions by reading the notebook.\n",
      "    *   All code is clean and free of debugging artifacts.\n",
      "    *   Key insights from statistical analysis and visualizations are explicitly stated in markdown.\n",
      "*   **Potential Challenges:**\n",
      "    *   Forgetting to document assumptions or specific data nuances.\n",
      "    *   Inconsistent documentation style.\n",
      "    *   Leaving in temporary or debugging code.\n",
      "*   **Error Handling Consideration:**\n",
      "    *   Emphasize the importance of running \"Restart Kernel and Run All Cells\" to ensure reproducibility and catch any hidden dependencies or state issues.\n",
      "    *   Advise using clear, concise language in markdown cells.\u001b[00m\n",
      "\n",
      "\n",
      "\n",
      "Generated Plan: As Project Manager and Central Planner, I have analyzed the objective to create a data analysis notebook. Below is a detailed, step-by-step plan designed for clear implementation by the Coder, while allowing for necessary flexibility and addressing potential challenges. Each step includes its purpose, inputs, outputs, success criteria, and error handling considerations.\n",
      "\n",
      "---\n",
      "\n",
      "### Project Plan: Data Analysis Notebook Creation\n",
      "\n",
      "**Overall Objective:** Create a data analysis notebook that loads a CSV file, performs basic statistical analysis, and creates visualizations.\n",
      "\n",
      "---\n",
      "\n",
      "**Step 1: Project Setup and Environment Configuration**\n",
      "\n",
      "*   **What needs to be done:**\n",
      "    *   Create a dedicated project directory.\n",
      "    *   Initialize a virtual environment (e.g., `venv` or `conda env`).\n",
      "    *   Install necessary Python libraries: `pandas` for data manipulation, `numpy` for numerical operations, `matplotlib` and `seaborn` for visualization.\n",
      "    *   Create an empty Jupyter Notebook file (e.g., `data_analysis_notebook.ipynb`).\n",
      "*   **Required Inputs:**\n",
      "    *   Python interpreter (pre-installed).\n",
      "    *   Access to a terminal/command prompt.\n",
      "*   **Expected Outputs:**\n",
      "    *   A project directory containing:\n",
      "        *   A configured virtual environment.\n",
      "        *   A `requirements.txt` file listing installed libraries and their versions.\n",
      "        *   An empty Jupyter Notebook file (`data_analysis_notebook.ipynb`).\n",
      "*   **Success Criteria:**\n",
      "    *   The virtual environment is activated successfully.\n",
      "    *   All specified libraries (pandas, numpy, matplotlib, seaborn) can be imported within a new cell in the Jupyter Notebook without errors.\n",
      "    *   The notebook file is accessible and runnable.\n",
      "*   **Potential Challenges:**\n",
      "    *   Conflicting library versions if not using a virtual environment.\n",
      "    *   Internet connectivity issues during package installation.\n",
      "    *   Incorrect path to Python executable or virtual environment activation script.\n",
      "*   **Error Handling Consideration:**\n",
      "    *   Provide clear instructions for virtual environment activation and package installation commands.\n",
      "    *   Suggest `pip install -r requirements.txt` for reproducibility.\n",
      "\n",
      "---\n",
      "\n",
      "**Step 2: Data Loading**\n",
      "\n",
      "*   **What needs to be done:**\n",
      "    *   Write code to load the specified CSV file into a pandas DataFrame.\n",
      "    *   Assume the CSV file is located in a known path (e.g., `data/input.csv`).\n",
      "*   **Required Inputs:**\n",
      "    *   Path to the CSV file (e.g., `./data/sales_data.csv`).\n",
      "    *   Knowledge of any specific CSV parameters (e.g., delimiter, header row presence, encoding). If not specified, assume default `pd.read_csv` behavior.\n",
      "*   **Expected Outputs:**\n",
      "    *   A pandas DataFrame object containing the loaded data.\n",
      "    *   Initial display of DataFrame head (`df.head()`) and information (`df.info()`) within the notebook.\n",
      "*   **Success Criteria:**\n",
      "    *   The `pd.read_csv()` function executes without raising an error.\n",
      "    *   `df.head()` displays the first few rows of data, confirming correct loading and column recognition.\n",
      "    *   `df.info()` shows the expected number of rows and columns, and initial data types.\n",
      "*   **Potential Challenges:**\n",
      "    *   `FileNotFoundError`: Incorrect file path or file not present.\n",
      "    *   `ParserError`: Malformed CSV (e.g., inconsistent delimiters, extra/missing columns in rows).\n",
      "    *   `UnicodeDecodeError`: Incorrect file encoding.\n",
      "    *   Large file size: Memory issues; may require `chunksize` or using Dask.\n",
      "*   **Error Handling Consideration:**\n",
      "    *   Implement a `try-except FileNotFoundError` block to gracefully handle missing files, providing a clear message to check the file path.\n",
      "    *   Suggest specifying `encoding` (e.g., `'utf-8'`, `'latin1'`) if `UnicodeDecodeError` occurs.\n",
      "    *   Advise checking the CSV delimiter (`sep` parameter) if parsing issues arise.\n",
      "\n",
      "---\n",
      "\n",
      "**Step 3: Initial Data Inspection and Cleaning**\n",
      "\n",
      "*   **What needs to be done:**\n",
      "    *   Inspect the DataFrame for missing values (`df.isnull().sum()`).\n",
      "    *   Check for duplicate rows (`df.duplicated().sum()`).\n",
      "    *   Review data types (`df.dtypes` or `df.info()`) and convert if necessary (e.g., object to numeric, object to datetime).\n",
      "    *   Handle missing values (e.g., drop rows/columns, impute with mean/median/mode, or flag). The specific strategy depends on data context, but a default approach (e.g., dropping rows with many missing values, or simple imputation for small counts) should be chosen.\n",
      "    *   Remove duplicate rows if identified.\n",
      "*   **Required Inputs:**\n",
      "    *   The pandas DataFrame loaded in Step 2.\n",
      "*   **Expected Outputs:**\n",
      "    *   A cleaned pandas DataFrame, `df_cleaned`, with appropriate data types and handled missing/duplicate values.\n",
      "    *   Summary outputs showing the state before and after cleaning (e.g., counts of nulls/duplicates).\n",
      "*   **Success Criteria:**\n",
      "    *   `df_cleaned.isnull().sum()` shows zeros (or acceptable low counts) for critical columns.\n",
      "    *   `df_cleaned.duplicated().sum()` shows zero.\n",
      "    *   Data types are consistent with the nature of the data (e.g., numerical columns are `int` or `float`, date columns are `datetime`).\n",
      "    *   The cleaning process is documented via comments or markdown cells.\n",
      "*   **Potential Challenges:**\n",
      "    *   Ambiguous missing value representations (e.g., \"N/A\", \"-\", \"Unknown\" instead of `NaN`).\n",
      "    *   Incorrect data type conversions leading to errors or data loss (e.g., `pd.to_numeric` on non-numeric strings).\n",
      "    *   Deciding on the best strategy for missing data imputation without domain knowledge.\n",
      "    *   Outliers that might require specific handling.\n",
      "*   **Error Handling Consideration:**\n",
      "    *   Use `errors='coerce'` in `pd.to_numeric` to turn non-convertible values into `NaN`, allowing for subsequent missing value handling.\n",
      "    *   Document all cleaning decisions and assumptions made in markdown cells.\n",
      "\n",
      "---\n",
      "\n",
      "**Step 4: Basic Statistical Analysis**\n",
      "\n",
      "*   **What needs to be done:**\n",
      "    *   Calculate descriptive statistics for numerical columns (e.g., mean, median, standard deviation, min, max, quartiles using `df.describe()`).\n",
      "    *   Perform value counts for key categorical columns (`df['column'].value_counts()`).\n",
      "    *   (Optional but recommended) Calculate correlations between numerical columns (`df.corr()`).\n",
      "*   **Required Inputs:**\n",
      "    *   The cleaned pandas DataFrame (`df_cleaned`) from Step 3.\n",
      "*   **Expected Outputs:**\n",
      "    *   Clearly displayed statistical summaries for numerical columns.\n",
      "    *   Frequency tables for categorical columns.\n",
      "    *   (Optional) A correlation matrix or specific correlation values.\n",
      "    *   Markdown cells explaining the insights derived from these statistics.\n",
      "*   **Success Criteria:**\n",
      "    *   `df_cleaned.describe()` output is displayed and shows sensible values for numerical columns.\n",
      "    *   `value_counts()` outputs are displayed for at least 2-3 relevant categorical columns.\n",
      "    *   No errors occur during statistical computations.\n",
      "    *   The output provides a basic understanding of the data's distribution and central tendencies.\n",
      "*   **Potential Challenges:**\n",
      "    *   Columns mistakenly identified as numerical or categorical, leading to incorrect analysis.\n",
      "    *   Extreme outliers skewing mean/standard deviation.\n",
      "    *   Interpreting statistics without domain context.\n",
      "*   **Error Handling Consideration:**\n",
      "    *   Ensure numerical operations are only applied to actual numerical columns. Use `df.select_dtypes(include=np.number)` for robustness.\n",
      "    *   Add comments or markdown cells to highlight any unexpected statistical results (e.g., very high standard deviation, unexpected min/max values).\n",
      "\n",
      "---\n",
      "\n",
      "**Step 5: Data Visualization**\n",
      "\n",
      "*   **What needs to be done:**\n",
      "    *   Create at least three different types of visualizations using `matplotlib` and/or `seaborn`.\n",
      "    *   Suggestions:\n",
      "        *   **Distribution Plot:** Histogram or Box plot for a key numerical variable.\n",
      "        *   **Categorical Distribution:** Bar plot for a key categorical variable's counts or its relation to a numerical variable (e.g., average sales per category).\n",
      "        *   **Relationship Plot:** Scatter plot between two numerical variables, or a heatmap for correlations.\n",
      "    *   Ensure all plots have clear titles, axis labels, and legends where appropriate.\n",
      "    *   Customize plot aesthetics for readability (e.g., figure size).\n",
      "*   **Required Inputs:**\n",
      "    *   The cleaned pandas DataFrame (`df_cleaned`) from Step 3.\n",
      "    *   Selected columns for visualization.\n",
      "*   **Expected Outputs:**\n",
      "    *   Multiple, well-rendered plots displayed inline within the Jupyter Notebook.\n",
      "    *   Plots are visually informative and convey insights.\n",
      "    *   (Optional) Plots saved as image files (e.g., PNG) to a `visualizations/` directory.\n",
      "*   **Success Criteria:**\n",
      "    *   All specified plot types are generated without errors.\n",
      "    *   Plots effectively illustrate distributions, relationships, or trends in the data.\n",
      "    *   Titles and labels are descriptive and easy to understand.\n",
      "    *   Plots are legible and not overly cluttered.\n",
      "*   **Potential Challenges:**\n",
      "    *   Choosing the most appropriate plot type for the data and insights.\n",
      "    *   Overplotting or unreadable plots with large datasets or many categories.\n",
      "    *   Misleading visualizations due to incorrect scaling or data aggregation.\n",
      "    *   Customization difficulties (e.g., rotating labels, adjusting figure size).\n",
      "*   **Error Handling Consideration:**\n",
      "    *   Check for empty or all-`NaN` columns before attempting to plot them.\n",
      "    *   Provide examples of common plot types and let the coder choose based on data characteristics.\n",
      "    *   Suggest using `plt.tight_layout()` to prevent labels from overlapping.\n",
      "\n",
      "---\n",
      "\n",
      "**Step 6: Notebook Documentation and Finalization**\n",
      "\n",
      "*   **What needs to be done:**\n",
      "    *   Add comprehensive markdown cells throughout the notebook to explain each step, the rationale behind decisions (e.g., cleaning choices), and interpretations of findings.\n",
      "    *   Add comments to complex or non-obvious code sections.\n",
      "    *   Ensure a logical flow from top to bottom.\n",
      "    *   Summarize key findings and insights at the end of the notebook.\n",
      "    *   Review the entire notebook for clarity, correctness, and reproducibility.\n",
      "*   **Required Inputs:**\n",
      "    *   The completed data analysis notebook (`data_analysis_notebook.ipynb`).\n",
      "*   **Expected Outputs:**\n",
      "    *   A self-contained, well-documented, and executable Jupyter Notebook.\n",
      "    *   A clear narrative of the data analysis process and its outcomes.\n",
      "*   **Success Criteria:**\n",
      "    *   The notebook can be executed from start to finish without errors.\n",
      "    *   Anyone unfamiliar with the project can understand the data, the analysis steps, and the conclusions by reading the notebook.\n",
      "    *   All code is clean and free of debugging artifacts.\n",
      "    *   Key insights from statistical analysis and visualizations are explicitly stated in markdown.\n",
      "*   **Potential Challenges:**\n",
      "    *   Forgetting to document assumptions or specific data nuances.\n",
      "    *   Inconsistent documentation style.\n",
      "    *   Leaving in temporary or debugging code.\n",
      "*   **Error Handling Consideration:**\n",
      "    *   Emphasize the importance of running \"Restart Kernel and Run All Cells\" to ensure reproducibility and catch any hidden dependencies or state issues.\n",
      "    *   Advise using clear, concise language in markdown cells.\n"
     ]
    }
   ],
   "source": [
    "def create_analysis_plan(objective):\n",
    "    \"\"\"\n",
    "    Creates a step-by-step plan to achieve the given objective.\n",
    "    \"\"\"\n",
    "    # Define the steps for data analysis\n",
    "    steps = [\n",
    "        {\n",
    "            \"step\": 1,\n",
    "            \"title\": \"Data Loading and Initial Setup\",\n",
    "            \"actions\": [\n",
    "                \"Import required libraries (pandas, numpy, matplotlib, seaborn)\",\n",
    "                \"Load the CSV file using pandas\",\n",
    "                \"Display basic information about the dataset\",\n",
    "                \"Handle any missing values or data cleaning if needed\"\n",
    "            ],\n",
    "            \"expected_output\": \"Loaded dataset with basic information displayed\"\n",
    "        },\n",
    "        {\n",
    "            \"step\": 2,\n",
    "            \"title\": \"Basic Statistical Analysis\",\n",
    "            \"actions\": [\n",
    "                \"Calculate summary statistics (mean, median, std, etc.)\",\n",
    "                \"Generate correlation matrix\",\n",
    "                \"Identify key patterns and trends\",\n",
    "                \"Document findings\"\n",
    "            ],\n",
    "            \"expected_output\": \"Statistical summary and correlation analysis\"\n",
    "        },\n",
    "        {\n",
    "            \"step\": 3,\n",
    "            \"title\": \"Data Visualization\",\n",
    "            \"actions\": [\n",
    "                \"Create distribution plots for key variables\",\n",
    "                \"Generate correlation heatmap\",\n",
    "                \"Create box plots for numerical variables\",\n",
    "                \"Add appropriate titles and labels\"\n",
    "            ],\n",
    "            \"expected_output\": \"Set of informative visualizations\"\n",
    "        },\n",
    "        {\n",
    "            \"step\": 4,\n",
    "            \"title\": \"Documentation and Finalization\",\n",
    "            \"actions\": [\n",
    "                \"Add markdown cells explaining the analysis\",\n",
    "                \"Include interpretation of results\",\n",
    "                \"Format the notebook for readability\",\n",
    "                \"Save the final notebook\"\n",
    "            ],\n",
    "            \"expected_output\": \"Complete, well-documented notebook\"\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    return steps\n",
    "\n",
    "# Test the planner with the example objective\n",
    "objective = \"\"\"Create a data analysis notebook that:\n",
    "    1. Loads a CSV file\n",
    "    2. Performs basic statistical analysis\n",
    "    3. Creates visualizations\"\"\"\n",
    "\n",
    "plan = create_analysis_plan(objective)\n",
    "if plan:\n",
    "    print(\"\\nGenerated Plan:\")\n",
    "    for step in plan:\n",
    "        print(f\"\\nStep {step['step']}: {step['title']}\")\n",
    "        print(\"Actions:\")\n",
    "        for action in step['actions']:\n",
    "            print(f\"- {action}\")\n",
    "        print(f\"Expected Output: {step['expected_output']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
